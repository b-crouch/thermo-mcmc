import emcee
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from multiprocessing import Pool

class MCMC:
    def __init__(self, prior, model, x_prop=None, y_prop=None, title=""):
        """
        Initialize MCMC object with specified model, likelihood, and prior functions
        Params:
            prior (dict): Dictionary indexed by parameter name in which values are SciPy.stats objects describing the prior distribution over the named parameter
            model (LogLikelihoodModel): likelihood model describing 1) how predictions are generated by a given parameter set omega and 2) the log-likelihood of the observed data
        """
        self.model = model
        self.check_model()
        self.prior = prior
        self.parameters = [str(param) for param in self.model.hessian_parameters]
        self.self_interaction = self.parameters[-1]

        # Store the non-Hessian parameters of the model
        for param in self.prior:
            if param not in self.parameters:
                self.parameters.append(param)

        self.n_parameters = len(self.parameters)
        self.latest_sample = None
        self.x_label = x_prop
        self.y_label = y_prop
        self.title = title

    def check_model(self):
        assert not self.model.is_multimodel, "Multi-model learning must be simulated using MultiModelMCMC object"

    def __repr__(self):
        return "MCMC()"

    def generate_starting_params(self, n_walkers):
        """
        Generate starting parameter values for an MCMC run by sampling over the distributions described in `self.prior`
        """
        p0 = np.zeros((n_walkers, self.n_parameters))
        for i, param in enumerate(self.prior):
            prior_dist = self.prior[param]
            p0[:, i] = prior_dist.rvs(size=n_walkers)
        return p0
    
    def log_prior(self, omega):
        """
        Compute the log-prior probability over the parameter set `omega`
        """
        log_prior = 0
        for i, param in enumerate(self.prior):
            prior_dist = self.prior[param]
            prior_prob = prior_dist.pdf(omega[i])
            if prior_prob == 0:
                log_prior = -np.inf
                break
            else:
                log_prior += np.log(prior_prob)
        return log_prior

    def log_posterior(self, omega, x, y):
        """
        Compute the log-posterior probabilty over the parameter set `omega` for the target variable `y` given input data `x`
        """
        log_posterior = self.model.gen_log_likelihood(omega, x, y) + self.log_prior(omega)
        if np.isnan(log_posterior):
            log_posterior = -np.inf
        return log_posterior

    def run(self, x, y, n_walkers, n_steps, verbose=True):
        """
        Run MCMC on provided data (x, y) using `n_walkers` chains each of `n_steps` sampling iterations
        """
        p0 = self.generate_starting_params(n_walkers)
        sampler = emcee.EnsembleSampler(n_walkers, self.n_parameters, self.log_posterior, args=[x, y])
        sampler.run_mcmc(p0, n_steps, progress=verbose)
        self.latest_sample = sampler.get_chain()
        return self.latest_sample
    
    def get_param_estimates(self, n_steps):
        """
        Return an estimate of each parameter by taking an average over the final `n_steps` of the trajectory for all walkers
        """
        assert self.latest_sample is not None, "Must call .run before estimating parameter values"
        estimates = np.mean(self.latest_sample[-n_steps:, :, :], axis=(0, 1))
        uncertainties = np.std(self.latest_sample[-n_steps:, :, :], axis=(0, 1))
        estimate_dict = {self.parameters[i]:(estimates[i], uncertainties[i]) for i in range(self.n_parameters)}
        return estimate_dict
    
    def compare_ground_truth(self, omega_true, n_steps):
        """
        Generate a DataFrame comparing the learned parameters to ground truth `omega_true`
        """
        mcmc_estimate = self.get_param_estimates(n_steps)
        return pd.DataFrame({param:[omega_true[param], mcmc_estimate[param][0], mcmc_estimate[param][1]] 
                      for param in self.parameters[:-1]}, index=["True", "MCMC Estimate", "MCMC SD"]).T

    def plot_trajectories(self, plot_conf=False):
        """
        Plot the sampling trajectory of each random walker in the most recent MCMC run
        """
        fig, ax = plt.subplots(1, self.n_parameters-1, figsize=(16, 3))
        plt.subplots_adjust(wspace=0.6)

        self_interaction_is_skipped = False
        for i, param in enumerate(self.parameters):
            if param != self.self_interaction:
                if not plot_conf:
                    for j in range(self.latest_sample.shape[1]):
                        ax[i - int(self_interaction_is_skipped)].plot(self.latest_sample[:, j, i - int(self_interaction_is_skipped)], alpha=0.5)
                else:
                    param_data = pd.DataFrame(self.latest_sample[:, :, i].T, columns=range(self.latest_sample.shape[0])).melt().rename(columns={"variable":"iteration"})
                    sns.lineplot(data=param_data, x="iteration", y="value", ax=ax[i - int(self_interaction_is_skipped)]);
                ax[i - int(self_interaction_is_skipped)].set_xlabel("MCMC Iteration", fontsize=14)
                ax[i - int(self_interaction_is_skipped)].set_ylabel(param, fontsize=18);
            else:
                self_interaction_is_skipped = True
                continue
        plt.suptitle(self.title, fontsize=15)
        return fig, ax
    
    def plot_distribution(self, burn_in=0, kind="arviz"):
        """
        Plot the sampling distribution over all parameters in the most recent MCMC run
        """
        if "a" in kind:
            # Seaborn plotting
            flattened_samples = self.latest_sample[burn_in:, :, :].reshape(-1, self.n_parameters)
            results_df = pd.DataFrame(flattened_samples, columns=self.parameters)
            results_df = results_df.drop(columns=self.self_interaction)
            with sns.plotting_context(rc={"axes.labelsize":20}):
                sns.pairplot(results_df, kind="kde", diag_kind="kde", corner=True);
        else:
            # Arviz plotting
            inf_data = az.convert_to_inference_data({param:self.latest_sample[:, :, i] for i, param in enumerate(self.parameters)})
            az.plot_pair(inf_data, 
                        kind="kde", 
                        scatter_kwargs={"s":50}, 
                        marginals=True, 
                        point_estimate="median", 
                        var_names=[fr"[^{self.self_interaction}]"], 
                        filter_vars="regex",
                        textsize=40);
        plt.suptitle(self.title)

    def plot_intermediates(self, walker_idx=0, n_plots=5, same_plot=True, x=None, y=None):
        """
        Visualize the learned property at `n_plots` points in the sample trajectory
        Params:
            learner_idx (int): Index of random walker in ensemble to use for plotting
            n_plots (int): Number of timesteps at which to generate plots
            same_plot (bool): If True, plots all intermediates on the same figure
            x (np.array): Ground truth frequencies
            y (np.array): Ground truth DOS
        """
        plt.figure(dpi=200)
        flattened_samples = self.latest_sample[:, walker_idx, :].reshape(-1, self.n_parameters)
        plot_idx = np.linspace(0, flattened_samples.shape[0]-1, n_plots).astype(int)
        plot_params = flattened_samples[plot_idx, :]
        cmap = sns.color_palette("viridis_r", len(plot_idx))
        for i in range(plot_params.shape[0]):
            omega = plot_params[i, :]
            label = f"Timestep {plot_idx[i]+1}/{flattened_samples.shape[0]}"
            if same_plot:
                self.model.plot(omega, self.x_label, self.y_label, label=label, c=cmap[i])
            else:
                plt.figure()
                omega = plot_params[i, :]
                self.model.plot(omega, self.x_label, self.y_label)
                plt.scatter(x, y, c="k")
                plt.title(label)
        if same_plot:
            plt.scatter(x, y, c="k", label="Observed Data")
            plt.title(self.title)
            plt.legend(bbox_to_anchor=(1,1));

    def get_rmse(self, walker_idx, x, y):
        """
        Compute the RMSE between the reconstructed quantity and observed `x` and `y`
        """
        omega = np.mean(self.latest_sample[-10:, walker_idx, :], axis=0)
        predictions = self.model.gen_prediction(omega, x)
        rmse = np.sqrt(np.mean((y - predictions)**2))
        return rmse

    def get_best_walker(self, x, y):
        """
        Return the index of the walker in the ensemble with lowest RMSE on the observed `x` and `y`
        """
        assert self.latest_sample is not None, "Must run random walk before evaluating results"
        n_walkers = self.latest_sample.shape[1]
        best_rmse, best_idx = np.inf, None
        for walker_idx in range(n_walkers):
            walker_rmse = self.get_rmse(walker_idx, x, y)
            if walker_rmse < best_rmse:
                best_rmse = walker_rmse
                best_idx = walker_idx
        return best_idx


class MultiModelMCMC(MCMC):
    """
    Child class to handle MCMC based on multiple sources of experimental data (eg phonon DOS and Cv curves)
    """
    def check_model(self):
        assert self.model.is_multimodel, "Single-model learning must be simulated using MCMC object"

    def log_posterior(self, omega, data):
        dataset_1, dataset_2 = data
        log_posterior = self.model.gen_log_likelihood(omega, data[dataset_1], data[dataset_2]) + self.log_prior(omega)
        if np.isnan(log_posterior):
            log_posterior = -np.inf
        return log_posterior
    
    def run(self, data, n_walkers, n_steps, verbose=True, pool=None):
        """
        Run MCMC on provided dictionary of data using `n_walkers` chains each of `n_steps` sampling iterations
        """
        p0 = self.generate_starting_params(n_walkers)
        if pool is not None:
            sampler = emcee.EnsembleSampler(n_walkers, self.n_parameters, self.log_posterior, args=[data], pool=pool)
        else:
            sampler = emcee.EnsembleSampler(n_walkers, self.n_parameters, self.log_posterior, args=[data])
        sampler.run_mcmc(p0, n_steps, progress=verbose)
        self.latest_sample = sampler.get_chain()
        return self.latest_sample
    
    def plot_intermediates(self, walker_idx=0, n_plots=5, true_data=None, true_data_format=None):
        flattened_samples = self.latest_sample[:, walker_idx, :].reshape(-1, self.n_parameters)
        plot_idx = np.linspace(0, flattened_samples.shape[0]-1, n_plots).astype(int)
        plot_params = flattened_samples[plot_idx, :]
        cmap = sns.color_palette("viridis_r", len(plot_idx))
        fig, ax = plt.subplots(1, 2, figsize=(10, 4))

        for i in range(plot_params.shape[0]):
            omega = plot_params[i, :]
            label = f"Timestep {plot_idx[i]+1}/{flattened_samples.shape[0]}"
            self.model.plot(omega, ax, c=cmap[i], label=label)

        if true_data is not None:
            dataset_1, dataset_2 = true_data
            if true_data_format is None:
                ax[0].scatter(true_data[dataset_1]["x"], true_data[dataset_1]["y"], c="k", label="Observed Data")
                ax[1].scatter(true_data[dataset_2]["x"], true_data[dataset_2]["y"], c="k", label="Observed Data")
            else:
                for i, dataset in enumerate(true_data):
                    if true_data_format[dataset] == "scatter":
                        ax[i].scatter(true_data[dataset]["x"], true_data[dataset]["y"], c="k", label="Observed Data")
                    elif true_data_format[dataset] == "line":
                        ax[i].plot(true_data[dataset]["x"], true_data[dataset]["y"], c="k", label="Observed Data")
        
        plt.suptitle(self.title)
        plt.legend(bbox_to_anchor=(1,1));

    def get_rmse(self, walker_idx, true_data):
        omega = np.mean(self.latest_sample[-10:, walker_idx, :], axis=0)
        dataset_1, dataset_2 = true_data
        predictions = self.model.gen_prediction(omega, true_data[dataset_1]["x"], true_data[dataset_2]["x"])
        n = len(predictions[dataset_1]) + len(predictions[dataset_2])
        rmse = np.sqrt((1/n) * (np.sum(((predictions[dataset_1] - true_data[dataset_1]["y"])/np.mean(true_data[dataset_1]["y"]))**2) \
                       + np.sum(((predictions[dataset_2] - true_data[dataset_2]["y"])/np.mean(true_data[dataset_1]["y"]))**2)))
        return rmse

    def get_best_walker(self, true_data):
        assert self.latest_sample is not None, "Must run random walk before evaluating results"
        n_walkers = self.latest_sample.shape[1]
        best_rmse, best_idx = np.inf, None
        for walker_idx in range(n_walkers):
            walker_rmse = self.get_rmse(walker_idx, true_data)
            if walker_rmse < best_rmse:
                best_rmse = walker_rmse
                best_idx = walker_idx
        return best_idx